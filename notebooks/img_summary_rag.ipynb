{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/anomaly_310/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ubuntu/anaconda3/envs/anomaly_310/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ubuntu/anaconda3/envs/anomaly_310/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../python'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from cvpr2018.utils.utils import register_logger\n",
    "from encoder.clip_encoder import ClipEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "log_every = 50  # log the writing of clips every n steps\n",
    "log_file = None  # set logging file\n",
    "num_workers = 4  # define the number of workers used for loading the videos\n",
    "\n",
    "cudnn.benchmark = True\n",
    "register_logger(log_file=log_file)\n",
    "\n",
    "dataset_path = '/home/ubuntu/repos/llm-rag/data/Anomaly-Videos-Part-1/test'  # path to the video dataset\n",
    "clip_length = 16  # define the length of each input sample\n",
    "frame_interval = 1 # define the sampling interval between framesq\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# llm.invoke(\"how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import clip\n",
    "# import numpy as np\n",
    "# from lavis.models import load_model_and_preprocess\n",
    "# from torchvision.transforms import ToPILImage\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "# from chromadb.utils.data_loaders import ImageLoader\n",
    "# import base64\n",
    "# from io import BytesIO\n",
    "\n",
    "# class ClipEncoder:\n",
    "#     def __init__(self, dataset_path, clip_length, caption_model_type, frame_interval, batch_size, num_workers):\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.dataset_path = dataset_path\n",
    "#         self.clip_length = clip_length\n",
    "#         self.frame_interval = frame_interval\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "#         self.data_loader, self.data_iter = get_features_loader(dataset_path,\n",
    "#                                                                     clip_length,\n",
    "#                                                                     frame_interval,\n",
    "#                                                                     batch_size,\n",
    "#                                                                     num_workers,\n",
    "#                                                                     \"clip\"\n",
    "#                                                                     )\n",
    "#         self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "#         self.caption_model, self.vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\",\n",
    "#                                                                                model_type=caption_model_type,\n",
    "#                                                                                is_eval=True,\n",
    "#                                                                                device=device)\n",
    "#     def encode_image(self, idx):\n",
    "#         frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "#         with torch.no_grad():\n",
    "#             frame_embeddings = self.model.encode_image(frame_tensor.cuda())\n",
    "#         return frame_embeddings\n",
    "\n",
    "#     def get_all_image_embeddings(self):\n",
    "#         embeddings = []\n",
    "#         for idx in range(len(self.data_loader)):\n",
    "#             emb = self.encode_image(idx)\n",
    "#             embeddings.append(emb)\n",
    "#         return embeddings\n",
    "    \n",
    "#     def export_tensor_to_np(self):\n",
    "#         arr = []\n",
    "#         for idx in range(len(self.data_loader)):\n",
    "#             frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "#             pil_image = ToPILImage()(frame_tensor[0]) \n",
    "#             arr.append(np.array(pil_image))\n",
    "#         return arr\n",
    "    \n",
    "#     def export_tensor_to_base64(self):\n",
    "#         arr = []\n",
    "#         for idx in range(len(self.data_loader)):\n",
    "#             frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "#             pil_image = ToPILImage()(frame_tensor[0])\n",
    "#             buffered = BytesIO()\n",
    "#             pil_image.save(buffered, format=\"JPEG\")\n",
    "#             img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "#             arr.append(img_str)\n",
    "#         return arr\n",
    "    \n",
    "#     def get_captions(self):\n",
    "#         captions_list = []  \n",
    "#         for idx in range(len(self.data_loader)):  \n",
    "#             frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "#             pil_image = ToPILImage()(frame_tensor[0]) \n",
    "#             image = self.vis_processors[\"eval\"](pil_image).unsqueeze(0).to(self.device)\n",
    "#             generated_captions = self.caption_model.generate({\"image\": image})  \n",
    "#             captions_list.append(generated_captions)\n",
    "#         return captions_list\n",
    "\n",
    "#     def get_all_caption_embeddings(self, captions_list):\n",
    "#         # Future improvements: Maybe multiple captions per image; Think about a way how to add anomalous captions / features\n",
    "#         caption_embeddings = []\n",
    "#         if captions_list:\n",
    "#             for caption_set in captions_list:\n",
    "#                 if caption_set:\n",
    "#                     for caption in caption_set:\n",
    "#                         if caption and len(caption) > 0:\n",
    "#                             with torch.no_grad():\n",
    "#                                 try:\n",
    "#                                     caption_features = clip.tokenize(caption).to(self.device)\n",
    "#                                     caption_embedding = self.model.encode_text(caption_features)\n",
    "#                                     caption_embeddings.append(caption_embedding)\n",
    "#                                 except Exception as e:\n",
    "#                                     print(f\"Error encoding text for caption: {caption}\")\n",
    "#                                     print(f\"Error details: {e}\")\n",
    "#         return caption_embeddings\n",
    "    \n",
    "#     def generate_document_ids(self):\n",
    "#         document_ids = []\n",
    "#         for i in range(len(self.data_loader)):\n",
    "#             item = self.data_loader.getitem_from_raw_video(idx=i)  \n",
    "#             for j in range(self.clip_length):\n",
    "#                 document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))\n",
    "\n",
    "#         batched_ids = [document_ids[i:i+clip_length] for i in range(0, len(document_ids), clip_length)]\n",
    "        \n",
    "#         return document_ids, batched_ids\n",
    "    \n",
    "#     def get_or_create_chroma_collection(self, collection_name, embedding_function=None, data_loader=None):\n",
    "#         if embedding_function:\n",
    "#             try:\n",
    "#                 collection = self.chroma_client.get_or_create_collection(name=collection_name, embedding_function=embedding_function, data_loader=data_loader)\n",
    "#                 return collection\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error creating collection: {collection_name}\")\n",
    "#                 print(f\"Error details: {e}\")\n",
    "#         else:    \n",
    "#             try:\n",
    "#                 collection = self.chroma_client.get_or_create_collection(collection_name)\n",
    "#                 return collection\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error creating collection: {collection_name}\")\n",
    "#                 print(f\"Error details: {e}\")\n",
    "\n",
    "\n",
    "#     def upload_embeddings_to_chroma(self, collection_name, img_data, ids, multi_modal= False, captions=None, documents=None, metadata=None):\n",
    "#         if multi_modal:\n",
    "#             if not len(img_data) == len(ids):\n",
    "#                 raise ValueError(\"data and ids must have the same length\")\n",
    "            \n",
    "#             embedding_function = OpenCLIPEmbeddingFunction(\"ViT-H-14\",\"laion2b_s32b_b79k\" )\n",
    "#             data_loader = ImageLoader()\n",
    "            \n",
    "#             collection = self.get_or_create_chroma_collection(collection_name, embedding_function, data_loader)\n",
    "#             print(\"Multi Modal Collection created\")\n",
    "\n",
    "#             for frame, id_ in zip(img_data, ids):\n",
    "#                 try:\n",
    "#                     collection.add(images=frame[0], metadatas=metadata, ids=id_[0])\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to add document with ID {id_}: {str(e)}\")         \n",
    "        \n",
    "#         else:\n",
    "#             if not len(img_data) == len(ids):\n",
    "#                 raise ValueError(\"data and ids must have the same length\")\n",
    "\n",
    "#             collection = self.get_or_create_chroma_collection(collection_name)\n",
    "\n",
    "#             for emb, id_ in zip(img_data, ids):\n",
    "#                 try:\n",
    "#                     collection.add(documents=documents, embeddings=emb, metadatas=metadata, ids=id_)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to add document with ID {id_}: {str(e)}\")                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-15 09:35:00,954 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-04-15 09:35:00,991 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-04-15 09:35:01,003 Found 6 video files in /home/ubuntu/repos/llm-rag/data/subset_normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23ffeca8a4e4f56ae42e63d9f7bea6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-15 09:35:32,431 Missing keys []\n",
      "2024-04-15 09:35:32,431 load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n",
      "2024-04-15 09:35:32,767 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-04-15 09:35:32,768 Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-04-15 09:35:32,777 Found 7 video files in /home/ubuntu/repos/llm-rag/data/subset_anomalous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-15 09:35:47,703 Missing keys []\n",
      "2024-04-15 09:35:47,704 load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n",
      "2024-04-15 09:35:47,704 load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n"
     ]
    }
   ],
   "source": [
    "normal_videos = \"/home/ubuntu/repos/llm-rag/data/subset_normal\"\n",
    "anomalous_videos = \"/home/ubuntu/repos/llm-rag/data/subset_anomalous\"\n",
    "normal_encoder = ClipEncoder(normal_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n",
    "anomalous_encoder = ClipEncoder(anomalous_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_caption_list = normal_encoder.get_captions()\n",
    "# anomalous_caption_list = anomalous_encoder.get_captions()\n",
    "\n",
    "normal_doc_ids = normal_encoder.generate_document_ids()\n",
    "anomalous_doc_ids = anomalous_encoder.generate_document_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Convert lists to dataframes\n",
    "# df_normal = pd.DataFrame(normal_caption_list, columns=['Captions'])\n",
    "# df_anomalous = pd.DataFrame(anomalous_caption_list, columns=['Captions'])\n",
    "\n",
    "# # Write dataframes to CSV\n",
    "# df_normal.to_csv('normal_captions.csv', index=False)\n",
    "# df_anomalous.to_csv('anomalous_captions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv to list of captions\n",
    "\n",
    "df_normal = pd.read_csv('normal_captions.csv')\n",
    "df_anomalous = pd.read_csv('weakly_labeled_anomalous_captions.csv', header=None)\n",
    "\n",
    "normal_caption_list = df_normal['Captions'].tolist()\n",
    "weakly_labeled_anomalous_caption_list = df_anomalous[0].tolist()\n",
    "\n",
    "#weakly_labeled_anomalous_caption_list = [str(anomalous_encoder.data_loader.getitem_from_raw_video(idx=i)[2]) + ' ' + caption for i, caption in enumerate(anomalous_caption_list)]\n",
    "\n",
    "#print(anomalous_encoder.data_loader.getitem_from_raw_video(idx=0)[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weakly_labeled_anomalous = pd.DataFrame(weakly_labeled_anomalous_caption_list, columns=['Captions'])\n",
    "df_weakly_labeled_anomalous.to_csv('weakly_labeled_anomalous_captions.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(normal_doc_ids[1]))\n",
    "print(len(weakly_labeled_anomalous_caption_list))\n",
    "\n",
    "print(normal_caption_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chroma retriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings()\n",
    "vectore_store = Chroma(collection_name='text_summary_db', persist_directory='/home/ubuntu/chroma_db/', embedding_function=embeddings)\n",
    "root_path = Path.cwd() / \"data\" / \"doc_store_text_summary\"\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = LocalFileStore(root_path)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectore_store,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "normal_summary_texts = [\n",
    "    Document(page_content=caption, metadata={id_key: doc_id})\n",
    "    for i, caption in enumerate(normal_caption_list)\n",
    "    for doc_id in normal_doc_ids[1][i]\n",
    "]\n",
    "\n",
    "anomalous_summary_texts = [\n",
    "    Document(page_content=caption, metadata={id_key: doc_id})\n",
    "    for i, caption in enumerate(weakly_labeled_anomalous_caption_list)\n",
    "    for doc_id in anomalous_doc_ids[1][i]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(normal_summary_texts)\n",
    "retriever.vectorstore.add_documents(anomalous_summary_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.docstore.mset(list(zip(normal_doc_ids[0], normal_summary_texts))) # Eventually run llava for long description and add to docstore\n",
    "# retriever.docstore.mset(list(zip(anomalous_doc_ids[0], anomalous_summary_texts))) \n",
    "\n",
    "import pickle\n",
    "\n",
    "retriever.docstore.mset([(doc_id, pickle.dumps(doc)) for doc_id, doc in zip(normal_doc_ids[0], normal_summary_texts)])\n",
    "retriever.docstore.mset([(doc_id, pickle.dumps(doc)) for doc_id, doc in zip(anomalous_doc_ids[0], anomalous_summary_texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"traffic\"\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which are short descriptions of videos:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "# Option 2: Multi-modal LLM\n",
    "# model = GPT4-V or LLaVA\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"Which document shows people walking next to lockers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"How many videos  show indoor scenes?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_encoder.chroma_client.delete_collection('text_summary_db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_310",
   "language": "python",
   "name": "anomaly_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
